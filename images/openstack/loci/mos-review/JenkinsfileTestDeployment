@Library("nc_cicd_internal_libs@e8f1b1d9aa34b8d5737f2a1d7e553824c4dfe4cc")

import groovy.json.JsonSlurperClassic
import com.att.nccicd.config.conf as config

conf = new config(env).CONF
json = new JsonSlurperClassic()

overrideImagesMap = json.parseText(OVERRIDE_IMAGES)

if (RELEASE == 'ocata') {
    DISTRO_VERSION = 'xenial'
} else if (RELEASE == 'stein') {
    DISTRO_VERSION = 'bionic'
}

def getProjectRepoUrl(prj) {
    return prj.contains("ssh://") ? prj : "${INTERNAL_GERRIT_SSH}/${prj}"
}

NET_RETRY_COUNT = NET_RETRY_COUNT.toInteger()
MANIFESTS_BRANCH = 'master'
MANIFESTS_PROJECT_NAME = conf.GLOBAL_REPO
VERSIONS_PATH = conf.VERSIONS_PATH
IMAGE_BASE_URL = String.format(conf.MOS_IMAGES_BASE_URL, "",RELEASE)
RELEASES_REGEX = "(${json.parseText(env.SUPPORTED_RELEASES).join("|")})"
RELEASE_OVERRIDES = conf.OSH_AIO_RELEASE_OVERRIDES
REPOS = conf.OSH_AIO_REPOS
BUILD_KUBEADM = false


def cloneOSH() {
    sh 'mkdir -p $WORKSPACE/artifacts'

    for (proj in ['openstack-helm', 'openstack-helm-infra']) {
        git_url = "${INTERNAL_GERRIT_SSH}/mirrors/opendev/${proj}.git"
        branch = "master"
        gerrit.cloneProject(git_url, branch, "", "${WORKSPACE}/${proj}", INTERNAL_GERRIT_KEY)
        version = gerrit.getVersion(git_url, branch, INTERNAL_GERRIT_KEY)
        sh "echo ${proj} head is at ${version} | tee -a ${WORKSPACE}/artifacts/OSH_version.txt"
    }
}


def imageOverrides(Map images) {
    imageTypes = ['nova', 'nova-1804', 'neutron', 'neutron-sriov', 'glance',
                  'cinder', 'heat', 'horizon', 'keystone']
    // populate images with default values
    imageTypes.each {
        key = "${it.replace('-', '_').toUpperCase()}_LOCI"
        if (images[key] == null) {
            images[key] = "${IMAGE_BASE_URL}/mos-${it}:latest"
        }
    }
    // replace upstream docker registries to artifactory cache
    ['docker\\.io', 'quay\\.io', 'k8s\\.gcr\\.io', 'gcr\\.io'].each {
        sh ("find . -type f -exec sed -i 's#${it}#${ARTF_DOCKER_URL}#g' {} +")
    }
    sh ("find . -type f -exec sed -i 's# \\(calico/ctl\\)# ${ARTF_DOCKER_URL}/\\1#g' {} +")

    utils.retrier (NET_RETRY_COUNT) {
        gerrit.cloneToBranch(
            getProjectRepoUrl(MANIFESTS_PROJECT_NAME),
            MANIFESTS_BRANCH,
            MANIFESTS_PROJECT_NAME,
            INTERNAL_GERRIT_KEY,
            MANIFESTS_BRANCH
        )
    }
    dir(MANIFESTS_PROJECT_NAME) {
        sh ("sed -i 's#DOCKER_OPEN_DOMAIN#${ARTF_DOCKER_URL}#g' ${VERSIONS_PATH}")
        sh ("sed -i 's#DOCKER_DOMAIN#${ARTF_SECURE_DOCKER_URL}#g' ${VERSIONS_PATH}")
        versions = readFile VERSIONS_PATH
        images.each { _, image ->
            (_, replace_to, pattern) = ((image =~ /.*?\/((.*)[@:].*)/)[0])
            // For pattern replace actual release name by regex matching any release
            pattern = pattern.replaceAll(RELEASES_REGEX, RELEASES_REGEX) + '[@:].*'
            versions = versions.replaceAll(pattern, replace_to)
        }
        writeFile file: VERSIONS_PATH, text: versions
        versionsData = (readYaml(file: VERSIONS_PATH))['data']['images']
    }

    def overrideYaml
    def releaseOverrides
    ['osh', 'ceph'].each {
        versionsData[it].each { chart, overrides ->
            if (fileExists("openstack-helm/${chart}")) {
                chartDir = "openstack-helm"
            } else if (fileExists("openstack-helm-infra/${chart}")) {
                chartDir = "openstack-helm-infra"
            } else { return }
            releaseOverrides = RELEASE_OVERRIDES[RELEASE][chart]
            if (releaseOverrides) {
                overrides << releaseOverrides
            }
            dir (chartDir) {
                overrideYaml = "${chart}/values_overrides/${RELEASE}-ubuntu_${DISTRO_VERSION}.yaml"
                sh "rm -rf ${overrideYaml}"
                writeYaml file: overrideYaml, data: ["images": ["tags": overrides]]
            }
        }
    }
    // update cirros image location to internal mirror to allow access from rally without proxy.
    // get glance test schema error while defining OSH_EXTRA_HELM_ARGS_GLANCE with --set overrides
    // replacing the url string for now
    sh """sed -i -e "s|http://download.cirros-cloud.net/0.3.5/|${conf.CIRROS_IMAGE_PATH}|" \\
          ${WORKSPACE}/openstack-helm/glance/values.yaml"""
}


def setproxy(){
    if (HTTP_PROXY){

        // redirection with "<<-" doesnot work well to remove whitespaces/tabs
        sh """sudo mkdir -p /etc/systemd/system/docker.service.d
             cat << EOF | sudo tee /etc/systemd/system/docker.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=${HTTP_PROXY}"
Environment="HTTPS_PROXY=${HTTP_PROXY}"
Environment="NO_PROXY=${NO_PROXY}"
EOF"""
        sh """cat << EOF | sudo tee -a /etc/environment
http_proxy=${HTTP_PROXY}
https_proxy=${HTTP_PROXY}
no_proxy=${NO_PROXY}
HTTP_PROXY=${HTTP_PROXY}
HTTPS_PROXY=${HTTP_PROXY}
NO_PROXY=${NO_PROXY}
EOF"""
        sh "sudo systemctl daemon-reload"
        sh "sudo systemctl restart docker"
        sh "export http_proxy=${HTTP_PROXY}"
        sh "export https_proxy=${HTTP_PROXY}"
        sh "export no_proxy=${NO_PROXY}"
        sh "export HTTP_PROXY=${HTTP_PROXY}"
        sh "export HTTPS_PROXY=${HTTP_PROXY}"
        sh "export NO_PROXY=${NO_PROXY}"
    }
}


def installDockerCE() {
    sh 'sudo apt-get remove -y runc containerd docker.io'
    packages = 'apt-transport-https ca-certificates curl software-properties-common'
    sh "sudo apt-get update && sudo apt-get upgrade -y; sudo apt-get install -y ${packages}"
    REPOS.each { component, data ->
        sh "sudo bash -c 'echo \"${data.source}\" >> /etc/apt/sources.list.d/${component}.list'"
        sh "sudo bash -c 'echo \"${data.pref}\" >> /etc/apt/preferences.d/${component}.pref'"
    }
    sh 'sudo apt-get update && sudo apt-get install -y docker-ce'
    sh "sudo systemctl daemon-reload"
    sh "sudo systemctl restart docker"
}


def tweakOSH() {
    // to remove once https://review.opendev.org/#/c/676823/ is merged
    sh 'sudo apt-get install -y bc'
    // to remove once https://review.opendev.org/#/c/675797/ is merged
    dir ("openstack-helm") { sh "git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/97/675797/6 && git checkout FETCH_HEAD" }
    // to remove once https://review.opendev.org/#/c/675789/ or https://review.opendev.org/#/c/675792/ are merged
    sh "sed -i 's/project_domain_id = \"\"/project_domain_id =/g' openstack-helm/glance/values.yaml"
    sh "sed -i 's/user_domain_id = \"\"/user_domain_id =/g' openstack-helm/glance/values.yaml"

    // to remove once https://review.opendev.org/#/c/675747/ is merged
    if (RELEASE == 'stein') {
        sh "sed -i 's/backup_driver: \"cinder.backup.drivers.swift\"/backup_driver: \"cinder.backup.drivers.swift.SwiftBackupDriver\"/g' openstack-helm/cinder/values.yaml"
    }
}

def TestVm(Map map, Closure body) {

    // Startup script to run after VM instance creation
    //  bootstrap.sh - default
    //  loci-bootstrap.sh - for loci builds
    def initScript = map.initScript ?: 'bootstrap.sh'

    // image used for creating instance
    def image = map.image ?: 'cicd-ubuntu-16.04-server-cloudimg-amd64'

    // flavor type used for creating instance
    def flavor = map.flavor ?: 'm1.medium'

    // postfix string for instance nodename
    def nodePostfix = map.nodePostfix ?: ''

    // build template used for heat stack creation
    //  basic - default
    //  loci - for loci builds
    def buildType = map.buildType ?: 'basic'

    // Flag to control node cleanup after job execution
    // Useful for retaining env for debugging failures
    // NodeCleanup job be used to destroy the node later
    //  false - default, deletes node after job
    //  true - do not delete node
    def doNotDeleteNode = map.doNotDeleteNode ?: false

    // Flag to control Jenkins console log publishing to Artifactory.
    //
    // This will also set custom URL to be returned when voting in Gerrit
    // https://jenkins.io/doc/pipeline/steps/gerrit-trigger/
    //
    // Useful for providing Jenkins console log when acting as 3rd party gate,
    // especially when Jenkins itself is not accessible
    def artifactoryLogs = map.artifactoryLogs ?: false

    // global timeout for executing pipeline
    // useful to prevent forever hanging pipelines consuming resources
    def globalTimeout = map.timeout ?: 120

    // Name of public network that is used to allocate floating IPs
    def publicNet = map.publicNet ?: 'public'

    // Name of private network for the VM
    def privateNet = map.privateNet ?: 'private'

    // resolve args to heat parameters
    def parameters = " --parameter image=${image}" +
                     " --parameter flavor=${flavor}" +
                     " --parameter public_net=${publicNet}" +
                     " --parameter private_net=${privateNet}"

    // node used for launching VMs
    def launch_node = 'jenkins-node-launch'

    def name = "${JOB_BASE_NAME}-${BUILD_NUMBER}"

    def postBuildBody = map.postBuildBody

    // templates located in resources from shared libraries
    // https://github.com/att-comdev/cicd/tree/master/resources
    def stack_template="heat/stack/ubuntu.${buildType}.stack.template.yaml"

    // optionally uer may supply additional identified for the VM
    // this makes it easier to find it in OpenStack (e.g. name)
    if (nodePostfix) {
      name += "-${nodePostfix}"
    }

    def ip = ""

    timestamps {
        try {
            stage ('Node Launch') {

                node(launch_node) {
                    tmpl = libraryResource "${stack_template}"
                    writeFile file: 'template.yaml', text: tmpl

                    if (initScript) {
                        data = libraryResource "heat/stack/${initScript}"
                        writeFile file: initScript, text: data
                    }

                    heat.stack_create(name, "${WORKSPACE}/template.yaml", parameters)
                    ip = heat.stack_output(name, 'floating_ip')
                }

                node('master') {
                    jenkins.node_create (name, ip)

                    timeout (14) {
                        node(name) {
                            sh 'cloud-init status --wait'
                        }
                    }
                }
            }

            // execute pipeline body, everything within vm()
            node (name) {
                try {
                    vm.message ('READY: JENKINS WORKER LAUNCHED') {
                        print "Launch overrides: ${map}\n" +
                              "Pipeline timeout: ${globalTimeout}\n" +
                              "Heat template: ${stack_template}\n" +
                              "Node IP: ${ip}"
                    }
                    timeout(globalTimeout) {
                        body()
                    }
                    vm.message ('SUCCESS: PIPELINE EXECUTION FINISHED') {}
                    currentBuild.result = 'SUCCESS'

                  // use Throwable to catch java.lang.NoSuchMethodError error
                } catch (Throwable err) {
                    vm.message ('FAILURE: PIPELINE EXECUTION HALTED') {
                        print "Pipeline body failed or timed out: ${err}.\n" +
                              'Likely gate reports failure.\n'
                    }
                    currentBuild.result = 'FAILURE'
                    throw err
                }
            }
            if (currentBuild.result == 'SUCCESS' && postBuildBody) {
                node(launch_node) {
                    postBuildBody(name)
                }
            }

          // use Throwable to catch java.lang.NoSuchMethodError error
        } catch (Throwable err) {
            vm.message ('ERROR: FAILED TO LAUNCH JENKINS WORKER') {
                print 'Failed to launch Jenkins VM/worker.\n' +
                      'Likely infra/template or config error.\n' +
                      "Error message: ${err}"
            }
            currentBuild.result = 'FAILURE'
            throw err

        } finally {
            if (!doNotDeleteNode) {
                node('master') {
                    jenkins.node_delete(name)
                }
                node(launch_node) {
                   heat.stack_delete(name)
                }
            }
        }
        return ip
    }
}

def installOSHAIO(List steps, concurrent=true) {
    // see https://docs.openstack.org/openstack-helm/latest/install/developer/index.html
    def deploy_steps = ['Packages'   : 'common/000-install-packages.sh',
                        'Kubernetes' : 'common/010-deploy-k8s.sh',
                        'Clients'    : 'common/020-setup-client.sh',
                        'Ingress'    : 'common/030-ingress.sh',
                        'Ceph'       : 'ceph/040-ceph.sh',
                        'Ceph NS'    : 'ceph/045-ceph-ns-activate.sh',
                        'MariaDB'    : 'ceph/050-mariadb.sh',
                        'RabbitMQ'   : 'ceph/060-rabbitmq.sh',
                        'Memcached'  : 'ceph/070-memcached.sh',
                        'Keystone'   : 'ceph/080-keystone.sh',
                        'Heat'       : 'ceph/090-heat.sh',
                        'Horizon'    : 'ceph/100-horizon.sh',
                        'Rados GW'   : 'ceph/110-ceph-radosgateway.sh',
                        'Glance'     : 'ceph/120-glance.sh',
                        'Cinder'     : 'ceph/130-cinder.sh',
                        'Openvswitch': 'ceph/140-openvswitch.sh',
                        'Libvirt'    : 'ceph/150-libvirt.sh',
                        'Compute Kit': 'ceph/160-compute-kit.sh',
                        'Gateway'    : 'ceph/170-setup-gateway.sh']

    deploymentEnv = [
        'OS_REGION_NAME=',
        'OS_USERNAME=',
        'OS_PASSWORD=',
        'OS_PROJECT_NAME=',
        'OS_PROJECT_DOMAIN_NAME=',
        'OS_USER_DOMAIN_NAME=',
        'OS_AUTH_URL=',
        "OPENSTACK_RELEASE=${RELEASE}",
        "CONTAINER_DISTRO_VERSION=${DISTRO_VERSION}",
    ]
    runningSet = [:]
    steps.each { it ->
        runningSet[it] = {
            withEnv(deploymentEnv) {
                print "Installing ${it}..."
                dir ('openstack-helm') {
                    sh "./tools/deployment/developer/${deploy_steps[it]}"
                }
            }
        }
    }
    if (concurrent) {
        parallel runningSet
    } else {
        runningSet.each { _, closure -> closure() }
    }
}


def createSnapshot = { stackName ->
    withCredentials([usernamePassword(credentialsId: 'jenkins-openstack-18',
                                      usernameVariable: 'OS_USERNAME',
                                      passwordVariable: 'OS_PASSWORD')]) {
        cmd = heat.openstack_cmd(
            "openstack stack resource show ${stackName} server -c physical_resource_id -f value"
        )
        serverId = sh(returnStdout: true, script: cmd).trim()
        cmd = heat.openstack_cmd(
            "openstack server image create ${serverId} --name OSH_AIO_INITIAL_DEPLOY --wait"
        )
        sh cmd
    }
}



if (!params.REUSE_SNAPSHOT) {
// OSH AIO requires min 8 cpus cores and 16 GB ram
TestVm(initScript: 'bootstrap.sh',
       image: 'cicd-ubuntu-16.04-server-cloudimg-amd64',
       flavor: 'm1.xlarge',
       nodePostfix: 'create-initial',
       buildType: 'basic',
       doNotDeleteNode: false,
       postBuildBody: createSnapshot
) {
    sh "sudo bash -c 'echo \"nameserver ${DNS_SERVER_2}\" > /etc/resolv.conf'"
    sh 'sudo bash -c \'echo "127.0.0.1 localhost" > /etc/hosts\''
    sh 'sudo bash -c \'echo "172.17.0.1 \$(hostname)" >> /etc/hosts\''
    stage('Setup proxy') {
        setproxy()
    }
    stage('Install docker-ce') {
        installDockerCE()
        // work around for yet not identified issue in osh
        // in case of new deployment if this line is not present
        // OSH adds '172.17.0.1 localhost', that breaks stuff
    }
    stage('Authenticate docker repo') {
        utils.retrier(NET_RETRY_COUNT) {
            osh.dockerAuth()
        }
    }
    stage('Clone OpenstackHelm') {
        cloneOSH()
        tweakOSH()
    }
    stage('Update OSH proxy') {
        updateProxy()
    }
    stage('Override images') {
        // Override default OSH images from global manifests, RELEASE_OVERRIDES,
        // latest mos set and OVERRIDE_IMAGES map and creates override yamls
        // for every component.
        // Also replaces all mentiones of upstream registries to artifactory cache
        imageOverrides(overrideImagesMap)
    }
    stage('Download precreated kubeadm-aio image') {
        if (!BUILD_KUBEADM) {
			// Pulls kubeadm image and disables it's build saving ~1h
			utils.retrier(NET_RETRY_COUNT) {
				sh "sudo docker pull ${conf.OSH_AIO_KUBEADM_IMAGE}"
			}
			sh "sudo docker tag ${conf.OSH_AIO_KUBEADM_IMAGE} openstackhelm/kubeadm-aio:dev"
			sh "sudo docker rmi ${conf.OSH_AIO_KUBEADM_IMAGE}"
			sh "echo '' > openstack-helm-infra/roles/build-images/tasks/kubeadm-aio.yaml"
        }
    }
    stage('Install OSH AIO') {
        try {
            installOSHAIO(['Packages', 'Kubernetes', 'Clients', 'Ingress',
                           'Ceph', 'Ceph NS', 'MariaDB', 'Memcached'],
                          concurrent=false)
        } catch (Exception exception) {
            osh.artifactLogs()
            error "OSH AIO deployment failed with exception $exception"
        }
    }
}
}


def runHelmTests(tests, run_from_root=false) {
    _sudo = ""                                                                 
    if (run_from_root) {                                                       
        _sudo = "sudo"                                                         
    }                                                                          
    sh 'mkdir -p $WORKSPACE/artifacts'                                         
    def results = []
    def runningSet = [:]                                                       
                                                                               
    tests.each { it ->                                                         
        runningSet[it] = {                                                     
            res = ""                                                           
            try {                                                              
                cmd = "${_sudo} helm test --debug ${it} --timeout 600 2>\\&1"
                res += sh (returnStdout: true, script: cmd)                    
            } catch (Exception e) {                                              
                cmd = "${_sudo} kubectl logs ${it}-test --namespace openstack 2>\\&1"
                res += sh (returnStdout: true, script: cmd)                    
                throw e
            }                                                                  
            results.add(res)                                                   
        }                                                                      
    }      
    parallel runningSet                                                        
    writeFile file: "${WORKSPACE}/artifacts/helm_tests.log", text: results.join("\n")
}


def updateProxy() {
    sh '''sed -i "/external_dns_nameservers:/a\\      - ${DNS_SERVER_2}\\n      - ${DNS_SERVER_1}" \
          ./openstack-helm-infra/tools/images/kubeadm-aio/assets/opt/playbooks/vars.yaml'''
    def amap = ['kubernetes_network_default_device': 'docker0',
                'gate_fqdn_test': 'true',
                'proxy': [ 'http': HTTP_PROXY, 'https': HTTP_PROXY, 'noproxy': NO_PROXY] ]
    sh 'rm -rf ./openstack-helm-infra/tools/gate/devel/local-vars.yaml'
    writeYaml file: './openstack-helm-infra/tools/gate/devel/local-vars.yaml', data: amap
}


// OSH AIO requires min 8 cpus cores and 16 GB ram
TestVm(initScript: '',
       image: 'OSH_AIO_INITIAL_DEPLOY',
       flavor: 'm1.xlarge',
       nodePostfix: 'deploy-osh-aio',
       buildType: 'basic',
       doNotDeleteNode: false,
) {
    cleanWs()
    sh 'sudo bash -c \'echo "127.0.0.1 localhost" > /etc/hosts\''
    sh 'sudo bash -c \'echo "172.17.0.1 \$(hostname)" >> /etc/hosts\''
    stage('Authenticate docker repo') {
        utils.retrier(NET_RETRY_COUNT) {
            osh.dockerAuth()
        }
    }
    stage('Clone OpenstackHelm') {
        cloneOSH()
        tweakOSH()
    }
    stage('Override images') {
        // Override default OSH images from global manifests, RELEASE_OVERRIDES,
        // latest mos set and OVERRIDE_IMAGES map and creates override yamls
        // for every component.
        // Also replaces all mentiones of upstream registries to artifactory cache
        imageOverrides(overrideImagesMap)
    }
    stage('Wait for k8s cluster') {
        ['kube-system', 'ceph', 'openstack'].each {
            sh "./openstack-helm/tools/deployment/common/wait-for-pods.sh ${it} 1800"
        }
    }
    stage('Install OSH AIO') {
        try {
            installOSHAIO(['Memcached', 'RabbitMQ'])
            installOSHAIO(['Keystone'])
            installOSHAIO(['Heat', 'Horizon', 'Rados GW'])
            installOSHAIO(['Glance', 'Cinder', 'Openvswitch'])
            installOSHAIO(['Libvirt'])
            installOSHAIO(['Compute Kit'])
            installOSHAIO(['Gateway'])
        } catch (Exception e) {
            osh.artifactLogs()
            error "OSH AIO deployment failed with exception ${e}"
        }
    }
    stage('Run Helm tests') {
        try {
            runHelmTests(['nova', 'cinder', 'glance', 'heat', 'keystone', 'neutron'])
        } catch (Exception e) {
            osh.artifactLogs()
            throw e
        }
    }
}
